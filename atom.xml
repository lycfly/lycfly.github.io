<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>love酒窝</title>
  
  <subtitle>LYC&#39;s Playgrand</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-22T12:31:39.454Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>LYC</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习——性能评估方法(2)</title>
    <link href="http://yoursite.com/2019/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95-2/"/>
    <id>http://yoursite.com/2019/01/22/机器学习——性能评估方法-2/</id>
    <published>2019-01-22T12:31:39.000Z</published>
    <updated>2019-01-22T12:31:39.454Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习——数据集划分法</title>
    <link href="http://yoursite.com/2019/01/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/"/>
    <id>http://yoursite.com/2019/01/11/机器学习——性能评估/</id>
    <published>2019-01-11T09:16:57.000Z</published>
    <updated>2019-01-23T07:50:39.888Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习——性能评估方法-1"><a href="#机器学习——性能评估方法-1" class="headerlink" title="机器学习——性能评估方法(1)"></a>机器学习——性能评估方法(1)</h1><p>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure).本节主要包含：查准率、查全率、F1、ROC、AUC以及代价敏感错误率曲线。</p><blockquote><p>——周志华老师机器学习西瓜书的一些总结与备忘。</p></blockquote><a id="more"></a><h2 id="2-3-2-查准率、查全率与F1"><a href="#2-3-2-查准率、查全率与F1" class="headerlink" title="2.3.2 查准率、查全率与F1"></a>2.3.2 查准率、查全率与F1</h2><p>对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive) 、假正例 (false positive) 、真反倒(true negative)、假反例 (false negative) 四种情形。用TP 、FP 、TN 、FN 表示。</p><h2 id="真实情况-预测结果-正例-反例-正例-TP-真正例-FN-假反例-反例-FP-假正例-TN-真反例"><a href="#真实情况-预测结果-正例-反例-正例-TP-真正例-FN-假反例-反例-FP-假正例-TN-真反例" class="headerlink" title="  真实情况 预测结果   正例 反例   正例 TP(真正例) FN(假反例)   反例 FP(假正例) TN(真反例)  "></a><table> <tr> <th rowspan="2">真实情况</th> <th colspan="2">预测结果</th> </tr> <tr> <td>正例</td> <td>反例</td> </tr> <tr> <td>正例</td> <td>TP(真正例)</td> <td>FN(假反例)</td> </tr> <tr> <td>反例</td> <td>FP(假正例)</td> <td>TN(真反例)</td> </tr> </table></h2><p>查准率 P与查全率 R 分别定义：<br>$$<br>P=\frac{TP}{TP+FP}<br>$$</p><p>$$<br>R = \frac{TP}{TP+FN}<br>$$</p><p><img src="/2019/01/11/机器学习——性能评估/2019-01-19_201900.png" alt="P-R曲线"></p><ul><li><p>若一个学 习器的 P-R 曲线被另一个学习器的曲线完全包住， 则可断言后者的性能优于前者。</p></li><li><p>如发生交叉，则一般难以断言孰优孰劣，只能在具体的查全与查准率下比较。</p></li><li><p>“平衡点”（Break-Event Point ,<strong>BEP</strong>）图中斜线就是一个比较模型优劣的度量方法。</p></li><li><p>F1度量（更常用）：<br>$$<br>F1 = \frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}=\frac{2\times TP}{FP+FN+2\times TP}<br>$$<br><em>F1</em> 是基于查准率与查全率的<strong>调和平均</strong> <em>(harinonicmean)</em>定义的:<br>$$<br>\frac{1}{F1}=\frac{1}{2}(\frac{1}{R}+\frac{1}{P})<br>$$<br> 然而在一些应用中，对查准率和查全率的重视程度有所不同.例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴的，此时查准率更重要;而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。<br>$F <em>\beta$ 是基于查准率与查全率的<strong>加权调和平均</strong>：<br>$$<br>F</em>\beta = \frac{(1+\beta^2)\times P \times R}{(\beta^2\times P)+R}<br>$$<br>$\beta&gt;0$度量查全与查准的相对重要性。$\beta&gt;1$时<strong>查全率</strong>有更大影响，$\beta&lt;1$<strong>查准率</strong>有更大影响。</p><p>有多个混淆矩阵时（比如多次测试，或者多分类任务中两两类别都有一个混淆矩阵）：</p><p>（1）各混淆矩阵上分别计算出查准率和查全率，记为 (P1， R1 ) … (Pn ， Rn) ， 再计算平均值，这样就得到”宏查准率” (macro-P) 、 “宏查全率” (macro-R) ，以及相应的”宏F1” (macro-F1).</p><p>​                            <img src="/2019/01/11/机器学习——性能评估/1548065402948.png" alt="1548065402948"></p><p> （2）还可先将各泪淆矩阵的对应元素进行平均，得到 TP 、 FP 、 TN 、 FN 的平均值，再基于这些平均值计算出”微查准率 “(microo-P) 、 “徽查全率”(micro-R)和”微F1” (micro-F1):</p><p><img src="/2019/01/11/机器学习——性能评估/1548065525482.png" alt="1548065525482"></p></li></ul><p><img src="/2019/01/11/机器学习——性能评估/1548065609925.png" alt="1548065609925"></p><h2 id="2-3-3-ROC-与-AUC"><a href="#2-3-3-ROC-与-AUC" class="headerlink" title="2.3.3 ROC 与 AUC"></a>2.3.3 ROC 与 AUC</h2><h3 id="ROC-Receiver-Operating-Characteristic"><a href="#ROC-Receiver-Operating-Characteristic" class="headerlink" title="ROC(Receiver Operating Characteristic)"></a>ROC(Receiver Operating Characteristic)</h3><p>​        很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阔值(threshold)进行比较，若大于阈值则分为正类，否则为反类.例如，神经网络在一般情形下是对每个测试样本预测出一个 [0.0 ，1.0] 之间的实值，然后将这个值与 0.5 进行比较，大于 0.5 则判为正例，否则为反例. </p><ul><li><p>ROC曲线绘制方法：根据概率预测结果，将测试样本进行排序，”最可能”是正例的排在最前面，”最不可能”是正例的排在最后面。分类过程就相当于在这个排序中以某个”<strong>==截断点==</strong>“ (cut point)将样本分为两部分，前一部分判作正例，后一部分则判作反例. </p></li><li><p>ROC 曲线的纵轴是”真正例率” (True Positive Rate，简称 <strong><em>==TPR==</em></strong>)，横轴是”假正例率” (False PositiveRate，简称 <strong><em>==FPR==</em></strong>)  </p></li></ul><p><img src="/2019/01/11/机器学习——性能评估/fafr.png" alt=""></p><p><img src="/2019/01/11/机器学习——性能评估/1548066781467.png" alt="1548066781467"></p><ul><li><p>绘图过程很简单:给定 m+ 个正例和m一 个反例。</p><ul><li><p>根据学习器预测结果对样例进行排序，把分类阔值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为 0 ， 在坐标 (0， 0) 处标记一个点</p></li><li><p>将分类阐值依次设为每个样例的预测值，即依次将每个样例划分为正例.设前一个标记点坐标为 (x， y) 。</p><ul><li><p>当前若为真正例，则对应标记点的坐标为 $$(x,y+\frac{1}{m^+})$$;当前若为假正例，则对应标记点的坐标为$$(x+\frac{1}{m^-},y)$$ ，然后用线段连接相邻点即可。</p></li><li><p>当出现a个正例与b个反例的预测值大小相等的情况下，则对应的坐标变为$$(x+\frac{b}{m^-},y+\frac{a}{m^+})$$，为一斜线段。</p></li></ul></li></ul></li></ul><h3 id="AUC-Area-Under-ROC-Curve"><a href="#AUC-Area-Under-ROC-Curve" class="headerlink" title="AUC (Area Under ROC Curve)"></a>AUC (Area Under ROC Curve)</h3><p>AUC 可通过对ROC曲线下各部分的面积求和而得。</p><ul><li><p>估算公式为：$记ROC曲线由｛(x_1,y_1),(x_2,y_2),\dots(x_n,y_n)｝$连接而成。则:</p><p>$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})$$</p></li></ul><p>AUC 考虑的是样本预测的排序质量，因此它与排序误差有紧密联系.给定 $m^+$个正例和 $m^-$个反例.令 $D^+$ 和 $D^-$分别表示正、反例集合，则排序”损失” (loss)定义为:</p><p><img src="/2019/01/11/机器学习——性能评估/1548072682669.png" alt="1548072682669"></p><p>$l_{rank}$ 对应的是 ROC 曲线之上的面积。关于这点，可以参考博文：</p><p><a href="https://blog.csdn.net/icefire_tyh/article/details/52065867" target="_blank" rel="noopener"></a></p><p>其中对该问题做了较为详细的论述：</p><p><img src="/2019/01/11/机器学习——性能评估/1548073004800.png" alt="1548073004800"></p><ul><li><p>从上ROC图中可以看出，折线每次向右(右上)延伸，表示扫描到了反例，折线上方对应的面积，就是该反例后面有多少个正例，每个正例是一个正方形，对应的面积是1。同位置上的正例是个三角形，对应的面积是0.5。最后再除以一个归一化系数。因此有：$AUC = 1-l_{rank}$</p><h4 id="编程实践：绘制ROC曲线以及计算AUC值通用程序"><a href="#编程实践：绘制ROC曲线以及计算AUC值通用程序" class="headerlink" title="编程实践：绘制ROC曲线以及计算AUC值通用程序"></a>编程实践：绘制ROC曲线以及计算AUC值通用程序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ROCandAUC</span><span class="params">(predits,labels)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(predits) != len(labels):</span><br><span class="line">        <span class="keyword">raise</span> <span class="string">'Predits and Labels Length Error !'</span></span><br><span class="line">    POSITIVE = sum(labels)          <span class="comment">#正例样本数</span></span><br><span class="line">    NEGITIVE = len(labels)-POSITIVE <span class="comment">#反例样本数</span></span><br><span class="line">    zipped = list(zip(predits,labels))</span><br><span class="line">    zipped.sort(reverse=<span class="keyword">False</span>,key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>]) <span class="comment">#按输出值大小排序</span></span><br><span class="line">    AUC = <span class="number">0</span></span><br><span class="line">    draw_point_x = [<span class="number">0</span>]</span><br><span class="line">    draw_point_y = [<span class="number">0</span>]</span><br><span class="line">    positive_num = <span class="number">0</span></span><br><span class="line">    negative_num = <span class="number">0</span></span><br><span class="line">    <span class="comment">#按顺序设置cut-point，遇到正例向上画线，反例向右画线</span></span><br><span class="line">    <span class="keyword">while</span> len(zipped)!=<span class="number">0</span>:</span><br><span class="line">        max_predit = zipped[<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">while</span> len(zipped)!=<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> zipped[<span class="number">-1</span>][<span class="number">0</span>]==max_predit:</span><br><span class="line">                <span class="keyword">if</span> zipped[<span class="number">-1</span>][<span class="number">1</span>]==<span class="number">1</span>:</span><br><span class="line">                    positive_num += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    negative_num += <span class="number">1</span></span><br><span class="line">                zipped.pop(<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        draw_point_y.append(positive_num/POSITIVE) </span><br><span class="line">        draw_point_x.append(negative_num/NEGITIVE)</span><br><span class="line">        <span class="comment">#计算AUC值</span></span><br><span class="line">        AUC += <span class="number">0.5</span>*(draw_point_x[<span class="number">-1</span>]-draw_point_x[<span class="number">-2</span>])*(draw_point_y[<span class="number">-1</span>]+draw_point_y[<span class="number">-2</span>])</span><br><span class="line"><span class="comment">#绘制曲线</span></span><br><span class="line">    plt.plot(draw_point_x,draw_point_y)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> AUC</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pre = [<span class="number">0.9</span>,<span class="number">0.89</span>,<span class="number">0.8</span>,<span class="number">0.8</span>,<span class="number">0.79</span>,<span class="number">0.79</span>,<span class="number">0.78</span>,<span class="number">0.78</span>,<span class="number">0.7</span>,<span class="number">0.7</span>,<span class="number">0.6</span>,<span class="number">0.6</span>,<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.4</span>,<span class="number">0.4</span>,<span class="number">0.4</span>,<span class="number">0.3</span>,<span class="number">0.2</span>]</span><br><span class="line">label = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">auc = ROCandAUC(pre,label)</span><br><span class="line">print(auc)</span><br></pre></td></tr></table></figure><p><img src="/2019/01/11/机器学习——性能评估/1548229822394.png" alt="1548229822394"></p></li></ul><h2 id="2-3-4代价敏感错误率与代价曲线"><a href="#2-3-4代价敏感错误率与代价曲线" class="headerlink" title="2.3.4代价敏感错误率与代价曲线"></a>2.3.4代价敏感错误率与代价曲线</h2><p>简而言之就是把FP以及FN的情况在计算损失时赋予了一定权重，来不同程度的惩罚相应类型的错误。</p><p><img src="/2019/01/11/机器学习——性能评估/1548073381185.png" alt="1548073381185"></p><ul><li><p>实际在意的是$cost_{01}与cost_{10}$的<strong>比值</strong>而非绝对值。</p></li><li><p>“<strong>代价敏感</strong>“ (cost-sensitive)错误率为：</p><p><img src="/2019/01/11/机器学习——性能评估/1548073509880.png" alt="1548073509880"></p></li><li><p>i、 j 取值不限于0 、 1 ，则可定义出多分类任务的代价敏感性能度量.</p></li><li><p>在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价 需要引入”<strong>代价曲线</strong>“ (cost curve) .其x轴为P(+)cost, y轴为归一化cost:</p><p><img src="/2019/01/11/机器学习——性能评估/1548075805818.png" alt="1548075805818"></p></li><li><p>其中（FNR = 1 - TPR ）。事实上，对于给定样本集（p确定）以及代价$cost_{01}与cost_{10}$，$P(+)<em>{cost}$可以写为$P(+)</em>{cost}=\frac{a}{a+b},其中a= p\times cost_{01},b=(1-p)\times cost_{10}$的形式。则有：</p></li></ul><p>​        $$cost_{norm}=(FNR-FPR)\times P(+)_{cost} + FPR$$</p><p>​       因此代价曲线的斜率即为$FNR-FPR$。</p><ul><li><p>绘制: ROC 由线上每点对应了代价平面上的一条线段 </p></li><li><p>设 ROC 曲线上点的坐标为 (TPR， FPR) ，则可相应计算出 FNR，然后在代价平面上绘制一条从 (0， FPR) 到 (1， FNR) 的线段</p></li><li><p>线段下的面积即表示了该条件下的期望总体代价;如此将 ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价。</p><p><img src="/2019/01/11/机器学习——性能评估/1548076647085.png" alt="1548076647085"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习——性能评估方法-1&quot;&gt;&lt;a href=&quot;#机器学习——性能评估方法-1&quot; class=&quot;headerlink&quot; title=&quot;机器学习——性能评估方法(1)&quot;&gt;&lt;/a&gt;机器学习——性能评估方法(1)&lt;/h1&gt;&lt;p&gt;对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure).本节主要包含：查准率、查全率、F1、ROC、AUC以及代价敏感错误率曲线。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;——周志华老师机器学习西瓜书的一些总结与备忘。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="搬砖" scheme="http://yoursite.com/categories/%E6%90%AC%E7%A0%96/"/>
    
    
      <category term="机器学习(ML)" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习——数据集划分法</title>
    <link href="http://yoursite.com/2019/01/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86%E6%B3%95/"/>
    <id>http://yoursite.com/2019/01/11/机器学习——数据集划分法/</id>
    <published>2019-01-11T09:16:57.000Z</published>
    <updated>2019-01-21T12:32:54.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习——数据集划分法"><a href="#机器学习——数据集划分法" class="headerlink" title="机器学习——数据集划分法"></a>机器学习——数据集划分法</h1><p>常用的数据集划分方法。</p><hr><blockquote><p>——周志华老师机器学习西瓜书的一些总结与备忘。</p><a id="more"></a></blockquote><p>[TOC]</p><h2 id="2-2训练与测试集的划分"><a href="#2-2训练与测试集的划分" class="headerlink" title="2.2训练与测试集的划分"></a>2.2训练与测试集的划分</h2><h3 id="2-2-1留出法-hold-out"><a href="#2-2-1留出法-hold-out" class="headerlink" title="2.2.1留出法 (hold-out)"></a>2.2.1留出法 (hold-out)</h3><p><img src="/2019/01/11/机器学习——数据集划分法/2.png" alt=""></p><ul><li>常见做法是将大约 $2/3、 4/5$ 的样本用于训练，剩余样本用于测试。</li><li><p>需要注意测试与训练的样本分布要尽量相同。</p><h3 id="2-2-2-交叉验证法-K-fold-cross-validation"><a href="#2-2-2-交叉验证法-K-fold-cross-validation" class="headerlink" title="2.2.2 交叉验证法 (K-fold cross validation)"></a>2.2.2 交叉验证法 (K-fold cross validation)</h3><p><img src="/2019/01/11/机器学习——数据集划分法/3.png" alt=""></p></li><li><p>为减小 因样本划分不同而引入的差别 ， k 折交叉验证通常要随机使用不同的划分重复 p 次。最终的评估结果是这 p 次 k 折交叉验证结果的均值，例如常见的有10 次 10 折交叉验证。（实践中这么多折相当耗费计算能力，感觉不太会进行重复的k折取平均）</p><h3 id="2-2-3留一法-Leave-One-Out-LOO"><a href="#2-2-3留一法-Leave-One-Out-LOO" class="headerlink" title="2.2.3留一法 (Leave-One-Out , LOO)"></a>2.2.3留一法 (Leave-One-Out , LOO)</h3></li><li>假定数据集 D 中包含 m 个样本 , 若令 k=m ， 则得到了交叉验证法的 一个特例。 </li><li>留一法不受样本随机划分方式的影响。（每折只包含一个测试样本，与直接用D进行训练的结果相似）</li><li>感觉实际用途不大。。。先不说训练的时候往往是根据划分的交叉验证集上的测试结果来评定本次训练的优劣。因此验证集直接决定了每一折保存的是训练过程中的哪个模型。只留下一个样本作为交叉验证没有意义，除非有另外的测试集。如果是这种情况，还不如直接把所有数据都作为训练集。如果是为了使用K-fold融合多个模型提升效果，使用留一的方法过于极端，样本数量大时模型数量过多，样本较少时可以考虑。<h3 id="2-2-4-自助法-bootstrapping"><a href="#2-2-4-自助法-bootstrapping" class="headerlink" title="2.2.4 自助法 (bootstrapping)"></a>2.2.4 自助法 (bootstrapping)</h3></li><li><p>但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集小，这必然会引入一些因训练样本规模不同而导致的估计偏差。</p><p><img src="/2019/01/11/机器学习——数据集划分法/1.png" alt=""></p></li><li><p>自助法在数据集较小、难以有效划分训练/测试集时很有用</p></li><li><p>此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处.</p></li><li><p>然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.</p></li></ul><blockquote><p>参考《周志华机器学习西瓜书》第二章</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习——数据集划分法&quot;&gt;&lt;a href=&quot;#机器学习——数据集划分法&quot; class=&quot;headerlink&quot; title=&quot;机器学习——数据集划分法&quot;&gt;&lt;/a&gt;机器学习——数据集划分法&lt;/h1&gt;&lt;p&gt;常用的数据集划分方法。&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;——周志华老师机器学习西瓜书的一些总结与备忘。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="搬砖" scheme="http://yoursite.com/categories/%E6%90%AC%E7%A0%96/"/>
    
    
      <category term="机器学习(ML)" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-ML/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow之Summary用法总结</title>
    <link href="http://yoursite.com/2018/12/16/Tensorflow%E4%B9%8BSummary%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/12/16/Tensorflow之Summary用法总结/</id>
    <published>2018-12-16T13:05:08.000Z</published>
    <updated>2018-12-19T05:26:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensorflow之Summary用法总结"><a href="#Tensorflow之Summary用法总结" class="headerlink" title="Tensorflow之Summary用法总结"></a>Tensorflow之Summary用法总结</h1><p>最近在研究tensorflow自带的例程speech_command,顺便学习tensorflow的一些基本用法。</p><p>其中tensorboard 作为一款可视化神器，可以说是学习tensorflow时模型训练以及参数可视化的法宝。</p><p>而在训练过程中，主要用到了tf.summary()的各类方法，能够保存训练过程以及参数分布图并在tensorboard显示。</p><a id="more"></a><h2 id="tf-summary包含的诸多函数"><a href="#tf-summary包含的诸多函数" class="headerlink" title="tf.summary包含的诸多函数"></a>tf.summary包含的诸多函数</h2><h3 id="1、tf-summary-scalar"><a href="#1、tf-summary-scalar" class="headerlink" title="1、tf.summary.scalar"></a>1、tf.summary.scalar</h3><p>用来显示标量信息，其格式为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(tags, values, collections=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>例如：tf.summary.scalar(‘mean’, mean)</p><p>一般在画loss,accuary时会用到这个函数。</p><h3 id="2、tf-summary-histogram"><a href="#2、tf-summary-histogram" class="headerlink" title="2、tf.summary.histogram"></a>2、tf.summary.histogram</h3><p>用来显示直方图信息，其格式为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(tags, values, collections=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>例如： tf.summary.histogram(‘histogram’, var)</p><p>一般用来显示训练过程中变量的分布情况</p><h3 id="3、tf-summary-distribution"><a href="#3、tf-summary-distribution" class="headerlink" title="3、tf.summary.distribution"></a>3、tf.summary.distribution</h3><p>分布图，一般用于显示weights分布</p><h3 id="4、tf-summary-text"><a href="#4、tf-summary-text" class="headerlink" title="4、tf.summary.text"></a>4、tf.summary.text</h3><p>可以将文本类型的数据转换为tensor写入summary中：</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"""/a/b/c\\_d/f\\_g\\_h\\_2017"""</span></span><br><span class="line">summary_op0 = tf.summary.text(<span class="string">'text'</span>, tf.convert_to_tensor(text))</span><br></pre></td></tr></table></figure><h3 id="5、tf-summary-image"><a href="#5、tf-summary-image" class="headerlink" title="5、tf.summary.image"></a>5、tf.summary.image</h3><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ tag /image/0’, ‘ tag /image/1’…，如：input/image/0等。</p><p>格式：tf.summary.image(tag, tensor, max_images=3, collections=None, name=Non</p><h3 id="6、tf-summary-audio"><a href="#6、tf-summary-audio" class="headerlink" title="6、tf.summary.audio"></a>6、tf.summary.audio</h3><p>展示训练过程中记录的音频 </p><h3 id="7、tf-summary-merge-all"><a href="#7、tf-summary-merge-all" class="headerlink" title="7、tf.summary.merge_all"></a>7、tf.summary.merge_all</h3><p>merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。</p><p>格式：tf.summaries.merge_all(key=’summaries’)</p><h3 id="8、tf-summary-FileWriter"><a href="#8、tf-summary-FileWriter" class="headerlink" title="8、tf.summary.FileWriter"></a>8、tf.summary.FileWriter</h3><p>指定一个文件用来保存图。</p><p>格式：tf.summary.FileWritter(path,sess.graph)</p><p>可以调用其add_summary（）方法将训练过程数据保存在filewriter指定的文件中</p><p>Tensorflow Summary 用法示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge_all()  </span><br><span class="line">train_writer = tf.summary.FileWriter(dir,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存</span></span><br></pre></td></tr></table></figure><p>此时开启tensorborad：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/summary_dir</span><br></pre></td></tr></table></figure><p>便能看见accuracy曲线了。</p><p>另外，如果我不想保存所有定义的summary信息，也可以用tf.summary.merge方法有选择性地保存信息：</p><h3 id="9、tf-summary-merge"><a href="#9、tf-summary-merge" class="headerlink" title="9、tf.summary.merge"></a>9、tf.summary.merge</h3><p>格式：tf.summary.merge(inputs, collections=None, name=None)</p><p>一般选择要保存的信息还需要用到tf.get_collection()函数</p><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">'accuracy'</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([tf.get_collection(tf.GraphKeys.SUMMARIES,<span class="string">'accuracy'</span>),...(其他要显示的信息)])  </span><br><span class="line">train_writer = tf.summary.FileWriter(dir,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存</span></span><br></pre></td></tr></table></figure><p>使用tf.get_collection函数筛选图中summary信息中的accuracy信息，这里的</p><p>tf.GraphKeys.SUMMARIES  是summary在collection中的标志。</p><p>当然，也可以直接：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">acc_summary = tf.summary.scalar(<span class="string">'accuracy'</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([acc_summary ,...(其他要显示的信息)])  <span class="comment">#这里的[]不可省</span></span><br></pre></td></tr></table></figure><p> 如果要在tensorboard中画多个数据图，需定义多个tf.summary.FileWriter并重复上述过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Tensorflow之Summary用法总结&quot;&gt;&lt;a href=&quot;#Tensorflow之Summary用法总结&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow之Summary用法总结&quot;&gt;&lt;/a&gt;Tensorflow之Summary用法总结&lt;/h1&gt;&lt;p&gt;最近在研究tensorflow自带的例程speech_command,顺便学习tensorflow的一些基本用法。&lt;/p&gt;
&lt;p&gt;其中tensorboard 作为一款可视化神器，可以说是学习tensorflow时模型训练以及参数可视化的法宝。&lt;/p&gt;
&lt;p&gt;而在训练过程中，主要用到了tf.summary()的各类方法，能够保存训练过程以及参数分布图并在tensorboard显示。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87/"/>
    
    
      <category term="Tensorflow" scheme="http://yoursite.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下QQ的使用并手动设置QQ文件保存路径</title>
    <link href="http://yoursite.com/2018/12/16/Ubuntu%E4%B8%8BQQ%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B9%B6%E6%89%8B%E5%8A%A8%E8%AE%BE%E7%BD%AEQQ%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E8%B7%AF%E5%BE%84/"/>
    <id>http://yoursite.com/2018/12/16/Ubuntu下QQ的使用并手动设置QQ文件保存路径/</id>
    <published>2018-12-16T12:51:26.000Z</published>
    <updated>2018-12-19T05:25:44.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景-amp-amp-目标"><a href="#背景-amp-amp-目标" class="headerlink" title="背景&amp;&amp;目标"></a>背景&amp;&amp;目标</h1><p>腾讯迟迟不肯做linux版本的QQ和微信，实在抠脚。<br>没有办法，要在linux上使用QQ，目前我找到最好的办法就是使用wine，然而wine这个杀千刀的又是个坑货，QQ除了聊天，还有最重要的功能就是传文件啊Orz,这货不但把路径隐藏了，还藏得这么深，，，无奈只能一层一层找，在用软连接链接出来。。。</p><a id="more"></a><p>下面主要以Ubuntu16.0.4为例，安装QQ,并手动设置文件保存路径。</p><h1 id="ubuntu下使用wine安装QQ"><a href="#ubuntu下使用wine安装QQ" class="headerlink" title="ubuntu下使用wine安装QQ"></a>ubuntu下使用wine安装QQ</h1><p> 主要参考 <a href="https://blog.csdn.net/hustcw98/article/details/79323024" target="_blank" rel="noopener">https://blog.csdn.net/hustcw98/article/details/79323024</a><br> 下载地址：<a href="http://yun.tzmm.com.cn/index.php/s/XRbfi6aOIjv5gwj" target="_blank" rel="noopener">http://yun.tzmm.com.cn/index.php/s/XRbfi6aOIjv5gwj</a><br> Appimage包不用做什么别的处理，安装啥的都不需要。。找到文件所在目录，终端中修改一下文件的权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x QQ-20171129-x86_64.AppImage</span><br></pre></td></tr></table></figure><p>之后就可以直接运行了。。。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./QQ-20171129-x86_64.AppImage</span><br></pre></td></tr></table></figure><p> 然而作为深度windows依赖患者，自然不会习惯开个qq还要敲命令<br> 索性在把它固定到开始栏：<br> 首先把QQ-20171129-x86_64.AppImage 名字改的简单点，移动到linux下的/opt下：<br> 先cd到QQ-20171129-x86_64.AppImage所在路径，之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv QQ-20171129-x86_64.AppImage /opt/QQ</span><br></pre></td></tr></table></figure><p>再创建个启动器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /usr/share/applications/QQ.desktop</span><br></pre></td></tr></table></figure><p>将以下内容复制进去：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry] </span><br><span class="line">Name=QQ</span><br><span class="line">Name[zh_CN]=QQ</span><br><span class="line">Exec=/opt/QQ</span><br><span class="line">Icon=/opt/QQ.png</span><br><span class="line">Terminal=false</span><br><span class="line">X-MultipleArgs=false</span><br><span class="line">Type=Application</span><br><span class="line">Encoding=UTF-8</span><br><span class="line">Categories=Application;</span><br><span class="line">StartupNotify=false</span><br></pre></td></tr></table></figure><p>其中，QQ.png图标可以从网上随便找一个图标放到/opt或者随便什么路径，只要desktop里填写正确路径即可。<br> 如此QQ就可以像windows里一样打开了，可能还要手动固定到任务栏，这个就不提了。</p><h1 id="创建QQ文件保存路径"><a href="#创建QQ文件保存路径" class="headerlink" title="创建QQ文件保存路径"></a>创建QQ文件保存路径</h1><p>这种方法安装的QQ实际是基于wine的。。。如果你在里面接收文件，想要找到路径，这货显示的是windows里一样的路径，还有什么还有“我的电脑”。。。linux里哪来这玩意<br> 所以实际他把存的文件放在了一个隐藏文件夹里，在home/你的用户名 目录下按CTRL+h 显示隐藏文件，找到里面一个叫</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.QQ.unionfs</span><br></pre></td></tr></table></figure><p>的文件夹，从QQ里接到的文件都放在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.QQ.unionfs/drive_c/users/你的用户名/My Documents/Tencent Files</span><br></pre></td></tr></table></figure><p>文件下了。<br> 所以可以在自己在外面创建一个该文件夹的软连接，方便找文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /home/你的用户名/.QQ.unionfs/drive_c/users/你的用户名/My\ Documents/Tencent\ Files /home/你的用户名/</span><br></pre></td></tr></table></figure><p>大功告成！如此便可在linux下愉快的使用QQ了！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;背景-amp-amp-目标&quot;&gt;&lt;a href=&quot;#背景-amp-amp-目标&quot; class=&quot;headerlink&quot; title=&quot;背景&amp;amp;&amp;amp;目标&quot;&gt;&lt;/a&gt;背景&amp;amp;&amp;amp;目标&lt;/h1&gt;&lt;p&gt;腾讯迟迟不肯做linux版本的QQ和微信，实在抠脚。&lt;br&gt;没有办法，要在linux上使用QQ，目前我找到最好的办法就是使用wine，然而wine这个杀千刀的又是个坑货，QQ除了聊天，还有最重要的功能就是传文件啊Orz,这货不但把路径隐藏了，还藏得这么深，，，无奈只能一层一层找，在用软连接链接出来。。。&lt;/p&gt;
    
    </summary>
    
      <category term="技术文" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E6%96%87/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
</feed>
